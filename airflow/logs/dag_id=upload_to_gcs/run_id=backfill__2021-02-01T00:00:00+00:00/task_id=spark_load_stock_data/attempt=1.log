[2023-06-15T09:54:22.672+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: upload_to_gcs.spark_load_stock_data backfill__2021-02-01T00:00:00+00:00 [queued]>
[2023-06-15T09:54:22.681+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: upload_to_gcs.spark_load_stock_data backfill__2021-02-01T00:00:00+00:00 [queued]>
[2023-06-15T09:54:22.681+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 2
[2023-06-15T09:54:22.711+0000] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): spark_load_stock_data> on 2021-02-01 00:00:00+00:00
[2023-06-15T09:54:22.716+0000] {standard_task_runner.py:57} INFO - Started process 8139 to run task
[2023-06-15T09:54:22.719+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'upload_to_gcs', 'spark_load_stock_data', 'backfill__2021-02-01T00:00:00+00:00', '--job-id', '419', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/upload_to_gcs.py', '--cfg-path', '/tmp/tmp0ubf224h']
[2023-06-15T09:54:22.719+0000] {standard_task_runner.py:85} INFO - Job 419: Subtask spark_load_stock_data
[2023-06-15T09:54:22.760+0000] {task_command.py:410} INFO - Running <TaskInstance: upload_to_gcs.spark_load_stock_data backfill__2021-02-01T00:00:00+00:00 [running]> on host 69b643fc97df
[2023-06-15T09:54:22.842+0000] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='upload_to_gcs' AIRFLOW_CTX_TASK_ID='spark_load_stock_data' AIRFLOW_CTX_EXECUTION_DATE='2021-02-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='backfill__2021-02-01T00:00:00+00:00'
[2023-06-15T09:54:22.853+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-06-15T09:54:22.855+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --name arrow-spark --verbose /opt/spark/app/load_data.py 2021-02-01 2efb4e0c80a041b794abaf4369e76869 dtc-de-382609_bucket
[2023-06-15T09:54:22.878+0000] {spark_submit.py:490} INFO - /opt/***/spark/spark-3.4.0-bin-hadoop3/bin/load-spark-env.sh: line 68: ps: command not found
[2023-06-15T09:54:24.283+0000] {spark_submit.py:490} INFO - Using properties file: null
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - Parsed arguments:
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - master                  spark://spark:7077
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - remote                  null
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - deployMode              null
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - executorMemory          null
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - executorCores           null
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - totalExecutorCores      null
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - propertiesFile          null
[2023-06-15T09:54:24.444+0000] {spark_submit.py:490} INFO - driverMemory            null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - driverCores             null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - driverExtraClassPath    null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - driverExtraLibraryPath  null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - driverExtraJavaOptions  null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - supervise               false
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - queue                   null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - numExecutors            null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - files                   null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - pyFiles                 null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - archives                null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - mainClass               null
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - primaryResource         file:/opt/spark/app/load_data.py
[2023-06-15T09:54:24.445+0000] {spark_submit.py:490} INFO - name                    arrow-spark
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - childArgs               [2021-02-01 2efb4e0c80a041b794abaf4369e76869 dtc-de-382609_bucket]
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - jars                    null
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - packages                null
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - packagesExclusions      null
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - repositories            null
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - verbose                 true
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - 
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - Spark properties used, including those specified through
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - --conf and those from the properties file null:
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - 
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - 
[2023-06-15T09:54:24.446+0000] {spark_submit.py:490} INFO - 
[2023-06-15T09:54:24.744+0000] {spark_submit.py:490} INFO - Main class:
[2023-06-15T09:54:24.744+0000] {spark_submit.py:490} INFO - org.apache.spark.deploy.PythonRunner
[2023-06-15T09:54:24.744+0000] {spark_submit.py:490} INFO - Arguments:
[2023-06-15T09:54:24.744+0000] {spark_submit.py:490} INFO - file:/opt/spark/app/load_data.py
[2023-06-15T09:54:24.744+0000] {spark_submit.py:490} INFO - null
[2023-06-15T09:54:24.744+0000] {spark_submit.py:490} INFO - 2021-02-01
[2023-06-15T09:54:24.745+0000] {spark_submit.py:490} INFO - 2efb4e0c80a041b794abaf4369e76869
[2023-06-15T09:54:24.745+0000] {spark_submit.py:490} INFO - dtc-de-382609_bucket
[2023-06-15T09:54:24.748+0000] {spark_submit.py:490} INFO - Spark config:
[2023-06-15T09:54:24.748+0000] {spark_submit.py:490} INFO - (spark.app.name,arrow-spark)
[2023-06-15T09:54:24.748+0000] {spark_submit.py:490} INFO - (spark.app.submitTime,1686822864726)
[2023-06-15T09:54:24.748+0000] {spark_submit.py:490} INFO - (spark.master,spark://spark:7077)
[2023-06-15T09:54:24.749+0000] {spark_submit.py:490} INFO - (spark.submit.deployMode,client)
[2023-06-15T09:54:24.749+0000] {spark_submit.py:490} INFO - (spark.submit.pyFiles,)
[2023-06-15T09:54:24.749+0000] {spark_submit.py:490} INFO - Classpath elements:
[2023-06-15T09:54:24.749+0000] {spark_submit.py:490} INFO - 
[2023-06-15T09:54:24.749+0000] {spark_submit.py:490} INFO - 
[2023-06-15T09:54:24.749+0000] {spark_submit.py:490} INFO - 
[2023-06-15T09:54:26.053+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SparkContext: Running Spark version 3.4.0
[2023-06-15T09:54:26.108+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-06-15T09:54:26.239+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO ResourceUtils: ==============================================================
[2023-06-15T09:54:26.239+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-06-15T09:54:26.240+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO ResourceUtils: ==============================================================
[2023-06-15T09:54:26.240+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SparkContext: Submitted application: SparkUploadData
[2023-06-15T09:54:26.271+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-06-15T09:54:26.286+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO ResourceProfile: Limiting resource is cpu
[2023-06-15T09:54:26.288+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-06-15T09:54:26.356+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SecurityManager: Changing view acls to: default
[2023-06-15T09:54:26.356+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SecurityManager: Changing modify acls to: default
[2023-06-15T09:54:26.357+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SecurityManager: Changing view acls groups to:
[2023-06-15T09:54:26.357+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SecurityManager: Changing modify acls groups to:
[2023-06-15T09:54:26.358+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[2023-06-15T09:54:26.686+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO Utils: Successfully started service 'sparkDriver' on port 40917.
[2023-06-15T09:54:26.722+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SparkEnv: Registering MapOutputTracker
[2023-06-15T09:54:26.763+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SparkEnv: Registering BlockManagerMaster
[2023-06-15T09:54:26.788+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-06-15T09:54:26.789+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-06-15T09:54:26.793+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-06-15T09:54:26.819+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e7c240a9-30a4-44aa-8175-ef098eba1f97
[2023-06-15T09:54:26.840+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-06-15T09:54:26.861+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:26 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-06-15T09:54:27.095+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-06-15T09:54:27.191+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-06-15T09:54:27.390+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2023-06-15T09:54:27.468+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO TransportClientFactory: Successfully created connection to spark/172.27.0.3:7077 after 59 ms (0 ms spent in bootstraps)
[2023-06-15T09:54:27.582+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20230615095427-0031
[2023-06-15T09:54:27.584+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230615095427-0031/0 on worker-20230615091312-172.27.0.2-45907 (172.27.0.2:45907) with 2 core(s)
[2023-06-15T09:54:27.587+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20230615095427-0031/0 on hostPort 172.27.0.2:45907 with 2 core(s), 1024.0 MiB RAM
[2023-06-15T09:54:27.592+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42739.
[2023-06-15T09:54:27.593+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO NettyBlockTransferService: Server created on 69b643fc97df:42739
[2023-06-15T09:54:27.596+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-06-15T09:54:27.608+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 69b643fc97df, 42739, None)
[2023-06-15T09:54:27.614+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO BlockManagerMasterEndpoint: Registering block manager 69b643fc97df:42739 with 434.4 MiB RAM, BlockManagerId(driver, 69b643fc97df, 42739, None)
[2023-06-15T09:54:27.616+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230615095427-0031/0 is now RUNNING
[2023-06-15T09:54:27.617+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 69b643fc97df, 42739, None)
[2023-06-15T09:54:27.619+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 69b643fc97df, 42739, None)
[2023-06-15T09:54:27.889+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:27 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2023-06-15T09:54:30.207+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-06-15T09:54:30.223+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:30 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-06-15T09:54:31.862+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:31 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.27.0.2:51386) with ID 0,  ResourceProfileId 0
[2023-06-15T09:54:31.974+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:31 INFO BlockManagerMasterEndpoint: Registering block manager 172.27.0.2:36507 with 434.4 MiB RAM, BlockManagerId(0, 172.27.0.2, 36507, None)
[2023-06-15T09:54:34.713+0000] {spark_submit.py:490} INFO - {'meta': {'symbol': 'AAPL', 'interval': '1h', 'currency': 'USD', 'exchange_timezone': 'America/New_York', 'exchange': 'NASDAQ', 'mic_code': 'XNGS', 'type': 'Common Stock'}, 'values': [{'datetime': '2021-02-01 15:30:00', 'open': '134.42999', 'high': '134.84000', 'low': '134.05000', 'close': '134.14000', 'volume': '9696032'}, {'datetime': '2021-02-01 14:30:00', 'open': '134.41000', 'high': '134.57001', 'low': '133.88000', 'close': '134.43010', 'volume': '8269109'}, {'datetime': '2021-02-01 13:30:00', 'open': '134.89200', 'high': '135.12000', 'low': '133.96001', 'close': '134.41000', 'volume': '9707748'}, {'datetime': '2021-02-01 12:30:00', 'open': '134.92999', 'high': '135.38000', 'low': '134.31000', 'close': '134.89040', 'volume': '11048977'}, {'datetime': '2021-02-01 11:30:00', 'open': '134.02100', 'high': '134.92999', 'low': '133.64999', 'close': '134.92010', 'volume': '12833301'}, {'datetime': '2021-02-01 10:30:00', 'open': '133.11549', 'high': '134.10001', 'low': '132.30000', 'close': '134.03000', 'volume': '15422047'}, {'datetime': '2021-02-01 09:30:00', 'open': '133.75000', 'high': '134.80000', 'low': '130.93150', 'close': '133.08000', 'volume': '30963937'}], 'status': 'ok'}
[2023-06-15T09:54:36.050+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO CodeGenerator: Code generated in 269.188862 ms
[2023-06-15T09:54:36.187+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-06-15T09:54:36.208+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2023-06-15T09:54:36.209+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-06-15T09:54:36.210+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO DAGScheduler: Parents of final stage: List()
[2023-06-15T09:54:36.213+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO DAGScheduler: Missing parents: List()
[2023-06-15T09:54:36.219+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-06-15T09:54:36.355+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.8 KiB, free 434.4 MiB)
[2023-06-15T09:54:36.413+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 434.4 MiB)
[2023-06-15T09:54:36.418+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 69b643fc97df:42739 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:36.427+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2023-06-15T09:54:36.455+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2023-06-15T09:54:36.457+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
[2023-06-15T09:54:36.499+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.27.0.2, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2023-06-15T09:54:36.504+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.27.0.2, executor 0, partition 1, PROCESS_LOCAL, 8088 bytes)
[2023-06-15T09:54:36.805+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.27.0.2:36507 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:38.829+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2343 ms on 172.27.0.2 (executor 0) (1/2)
[2023-06-15T09:54:38.830+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2327 ms on 172.27.0.2 (executor 0) (2/2)
[2023-06-15T09:54:38.832+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-06-15T09:54:38.841+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 46735
[2023-06-15T09:54:38.848+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.610 s
[2023-06-15T09:54:38.851+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-06-15T09:54:38.852+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-06-15T09:54:38.854+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:38 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.666371 s
[2023-06-15T09:54:40.348+0000] {spark_submit.py:490} INFO - {'meta': {'symbol': 'MSFT', 'interval': '1h', 'currency': 'USD', 'exchange_timezone': 'America/New_York', 'exchange': 'NASDAQ', 'mic_code': 'XNGS', 'type': 'Common Stock'}, 'values': [{'datetime': '2021-02-01 15:30:00', 'open': '240.44000', 'high': '240.56000', 'low': '239.42999', 'close': '239.64999', 'volume': '3385613'}, {'datetime': '2021-02-01 14:30:00', 'open': '240.92000', 'high': '241.39000', 'low': '240.24001', 'close': '240.45000', 'volume': '2613911'}, {'datetime': '2021-02-01 13:30:00', 'open': '241.17000', 'high': '241.67900', 'low': '240.57001', 'close': '240.92000', 'volume': '2402745'}, {'datetime': '2021-02-01 12:30:00', 'open': '242.47000', 'high': '242.50000', 'low': '240.52000', 'close': '241.17000', 'volume': '3316801'}, {'datetime': '2021-02-01 11:30:00', 'open': '237.99001', 'high': '242.50000', 'low': '237.96001', 'close': '242.46800', 'volume': '6112490'}, {'datetime': '2021-02-01 10:30:00', 'open': '235.03000', 'high': '238.02000', 'low': '235.03000', 'close': '237.97000', 'volume': '4928506'}, {'datetime': '2021-02-01 09:30:00', 'open': '235.16000', 'high': '235.64999', 'low': '232.46001', 'close': '235.00000', 'volume': '6360008'}], 'status': 'ok'}
[2023-06-15T09:54:40.507+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-06-15T09:54:40.509+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2023-06-15T09:54:40.510+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-06-15T09:54:40.510+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Parents of final stage: List()
[2023-06-15T09:54:40.510+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Missing parents: List()
[2023-06-15T09:54:40.514+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[22] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-06-15T09:54:40.522+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.8 KiB, free 434.4 MiB)
[2023-06-15T09:54:40.529+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 434.3 MiB)
[2023-06-15T09:54:40.532+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 69b643fc97df:42739 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:40.533+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2023-06-15T09:54:40.536+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[22] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2023-06-15T09:54:40.536+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2023-06-15T09:54:40.539+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.27.0.2, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2023-06-15T09:54:40.540+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.27.0.2, executor 0, partition 1, PROCESS_LOCAL, 8084 bytes)
[2023-06-15T09:54:40.585+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.27.0.2:36507 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:40.694+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 155 ms on 172.27.0.2 (executor 0) (1/2)
[2023-06-15T09:54:40.707+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 166 ms on 172.27.0.2 (executor 0) (2/2)
[2023-06-15T09:54:40.707+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-06-15T09:54:40.709+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.193 s
[2023-06-15T09:54:40.710+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-06-15T09:54:40.710+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-06-15T09:54:40.711+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:40 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.202912 s
[2023-06-15T09:54:42.041+0000] {spark_submit.py:490} INFO - {'meta': {'symbol': 'GOOG', 'interval': '1h', 'currency': 'USD', 'exchange_timezone': 'America/New_York', 'exchange': 'NASDAQ', 'mic_code': 'XNGS', 'type': 'Common Stock'}, 'values': [{'datetime': '2021-02-01 15:30:00', 'open': '1901.34998', 'high': '1906.06006', 'low': '1898.40002', 'close': '1901.34998', 'volume': '176950'}, {'datetime': '2021-02-01 14:30:00', 'open': '1913.06006', 'high': '1913.18909', 'low': '1900.90015', 'close': '1901.42004', 'volume': '117462'}, {'datetime': '2021-02-01 13:30:00', 'open': '1919.78223', 'high': '1922.39185', 'low': '1904.93079', 'close': '1911.90002', 'volume': '136168'}, {'datetime': '2021-02-01 12:30:00', 'open': '1910.55005', 'high': '1919.72998', 'low': '1906.70007', 'close': '1918.80994', 'volume': '176965'}, {'datetime': '2021-02-01 11:30:00', 'open': '1888.72998', 'high': '1913.77734', 'low': '1887.38000', 'close': '1910.00000', 'volume': '282128'}, {'datetime': '2021-02-01 10:30:00', 'open': '1872.00000', 'high': '1886.31812', 'low': '1862.34106', 'close': '1884.75000', 'volume': '173779'}, {'datetime': '2021-02-01 09:30:00', 'open': '1852.08997', 'high': '1875.87500', 'low': '1851.52002', 'close': '1870.97498', 'volume': '360385'}], 'status': 'ok'}
[2023-06-15T09:54:42.151+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-06-15T09:54:42.154+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2023-06-15T09:54:42.155+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-06-15T09:54:42.155+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Parents of final stage: List()
[2023-06-15T09:54:42.155+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Missing parents: List()
[2023-06-15T09:54:42.157+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[33] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-06-15T09:54:42.165+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.8 KiB, free 434.3 MiB)
[2023-06-15T09:54:42.169+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 434.3 MiB)
[2023-06-15T09:54:42.170+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 69b643fc97df:42739 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:42.171+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
[2023-06-15T09:54:42.172+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[33] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2023-06-15T09:54:42.173+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2023-06-15T09:54:42.175+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (172.27.0.2, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2023-06-15T09:54:42.175+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (172.27.0.2, executor 0, partition 1, PROCESS_LOCAL, 8105 bytes)
[2023-06-15T09:54:42.194+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.27.0.2:36507 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:42.260+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 86 ms on 172.27.0.2 (executor 0) (1/2)
[2023-06-15T09:54:42.261+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 85 ms on 172.27.0.2 (executor 0) (2/2)
[2023-06-15T09:54:42.261+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-06-15T09:54:42.264+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.105 s
[2023-06-15T09:54:42.265+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-06-15T09:54:42.266+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-06-15T09:54:42.267+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.114119 s
[2023-06-15T09:54:42.391+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 69b643fc97df:42739 in memory (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:42.398+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.27.0.2:36507 in memory (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:42.440+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 69b643fc97df:42739 in memory (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:42.444+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.27.0.2:36507 in memory (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:42.473+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 69b643fc97df:42739 in memory (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:42.477+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.27.0.2:36507 in memory (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:43.729+0000] {spark_submit.py:490} INFO - {'meta': {'symbol': 'AMZN', 'interval': '1h', 'currency': 'USD', 'exchange_timezone': 'America/New_York', 'exchange': 'NASDAQ', 'mic_code': 'XNGS', 'type': 'Common Stock'}, 'values': [{'datetime': '2021-02-01 15:30:00', 'open': '3327.35742', 'high': '3350.00000', 'low': '3325.29004', 'close': '3342.87988', 'volume': '679899'}, {'datetime': '2021-02-01 14:30:00', 'open': '3322.48999', 'high': '3330.83008', 'low': '3316.52002', 'close': '3326.79419', 'volume': '375100'}, {'datetime': '2021-02-01 13:30:00', 'open': '3313.84009', 'high': '3324.26807', 'low': '3306.33008', 'close': '3323.00000', 'volume': '291152'}, {'datetime': '2021-02-01 12:30:00', 'open': '3317.00000', 'high': '3319.89990', 'low': '3310.39014', 'close': '3313.47314', 'volume': '294975'}, {'datetime': '2021-02-01 11:30:00', 'open': '3313.86011', 'high': '3318.07910', 'low': '3296.61011', 'close': '3315.00000', 'volume': '410506'}, {'datetime': '2021-02-01 10:30:00', 'open': '3292.00000', 'high': '3314.40991', 'low': '3283.84009', 'close': '3312.50049', 'volume': '631432'}, {'datetime': '2021-02-01 09:30:00', 'open': '3244.98999', 'high': '3293.12988', 'low': '3235.02490', 'close': '3290.31006', 'volume': '1058486'}], 'status': 'ok'}
[2023-06-15T09:54:43.915+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-06-15T09:54:43.918+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2023-06-15T09:54:43.919+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2023-06-15T09:54:43.919+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO DAGScheduler: Parents of final stage: List()
[2023-06-15T09:54:43.919+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO DAGScheduler: Missing parents: List()
[2023-06-15T09:54:43.923+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[44] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-06-15T09:54:43.934+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.8 KiB, free 434.4 MiB)
[2023-06-15T09:54:43.936+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 434.4 MiB)
[2023-06-15T09:54:43.937+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 69b643fc97df:42739 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:43.938+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2023-06-15T09:54:43.940+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[44] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2023-06-15T09:54:43.941+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2023-06-15T09:54:43.943+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6) (172.27.0.2, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2023-06-15T09:54:43.944+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7) (172.27.0.2, executor 0, partition 1, PROCESS_LOCAL, 8106 bytes)
[2023-06-15T09:54:43.969+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.27.0.2:36507 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:44.030+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:44 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 88 ms on 172.27.0.2 (executor 0) (1/2)
[2023-06-15T09:54:44.032+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:44 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 88 ms on 172.27.0.2 (executor 0) (2/2)
[2023-06-15T09:54:44.032+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-06-15T09:54:44.033+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:44 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0.107 s
[2023-06-15T09:54:44.033+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:44 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-06-15T09:54:44.034+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-06-15T09:54:44.034+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:44 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0.117780 s
[2023-06-15T09:54:45.265+0000] {spark_submit.py:490} INFO - {'meta': {'symbol': 'TSLA', 'interval': '1h', 'currency': 'USD', 'exchange_timezone': 'America/New_York', 'exchange': 'NASDAQ', 'mic_code': 'XNGS', 'type': 'Common Stock'}, 'values': [{'datetime': '2021-02-01 15:30:00', 'open': '834.53998', 'high': '842.00000', 'low': '834.36011', 'close': '840.01660', 'volume': '2293103'}, {'datetime': '2021-02-01 14:30:00', 'open': '832.76001', 'high': '837.00000', 'low': '830.29999', 'close': '834.70990', 'volume': '1734107'}, {'datetime': '2021-02-01 13:30:00', 'open': '827.71997', 'high': '835.54962', 'low': '825.25000', 'close': '832.78992', 'volume': '2001443'}, {'datetime': '2021-02-01 12:30:00', 'open': '834.07001', 'high': '838.88000', 'low': '827.20001', 'close': '827.60999', 'volume': '2661481'}, {'datetime': '2021-02-01 11:30:00', 'open': '817.42999', 'high': '836.00000', 'low': '816.00000', 'close': '833.88000', 'volume': '4728107'}, {'datetime': '2021-02-01 10:30:00', 'open': '813.98999', 'high': '818.25000', 'low': '809.57617', 'close': '817.33002', 'volume': '3259670'}, {'datetime': '2021-02-01 09:30:00', 'open': '814.28998', 'high': '825.00000', 'low': '796.00000', 'close': '813.50049', 'volume': '7885834'}], 'status': 'ok'}
[2023-06-15T09:54:45.324+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-06-15T09:54:45.325+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Got job 4 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2023-06-15T09:54:45.326+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Final stage: ResultStage 4 (json at NativeMethodAccessorImpl.java:0)
[2023-06-15T09:54:45.326+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Parents of final stage: List()
[2023-06-15T09:54:45.326+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Missing parents: List()
[2023-06-15T09:54:45.327+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[55] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-06-15T09:54:45.333+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.8 KiB, free 434.4 MiB)
[2023-06-15T09:54:45.336+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 434.3 MiB)
[2023-06-15T09:54:45.337+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 69b643fc97df:42739 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:45.339+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2023-06-15T09:54:45.340+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[55] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2023-06-15T09:54:45.341+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2023-06-15T09:54:45.351+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8) (172.27.0.2, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2023-06-15T09:54:45.352+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9) (172.27.0.2, executor 0, partition 1, PROCESS_LOCAL, 8084 bytes)
[2023-06-15T09:54:45.375+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.27.0.2:36507 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:45.443+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 100 ms on 172.27.0.2 (executor 0) (1/2)
[2023-06-15T09:54:45.448+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 9) in 104 ms on 172.27.0.2 (executor 0) (2/2)
[2023-06-15T09:54:45.448+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-06-15T09:54:45.451+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: ResultStage 4 (json at NativeMethodAccessorImpl.java:0) finished in 0.121 s
[2023-06-15T09:54:45.452+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-06-15T09:54:45.453+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-06-15T09:54:45.453+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:45 INFO DAGScheduler: Job 4 finished: json at NativeMethodAccessorImpl.java:0, took 0.129033 s
[2023-06-15T09:54:46.812+0000] {spark_submit.py:490} INFO - {'meta': {'symbol': 'META', 'interval': '1h', 'currency': 'USD', 'exchange_timezone': 'America/New_York', 'exchange': 'NASDAQ', 'mic_code': 'XNGS', 'type': 'Common Stock'}, 'values': [{'datetime': '2021-02-01 15:30:00', 'open': '262.88000', 'high': '263.75000', 'low': '261.66501', 'close': '261.98999', 'volume': '2129116'}, {'datetime': '2021-02-01 14:30:00', 'open': '263.38989', 'high': '264.11990', 'low': '262.70999', 'close': '262.90500', 'volume': '1655456'}, {'datetime': '2021-02-01 13:30:00', 'open': '263.00000', 'high': '264.17001', 'low': '262.66000', 'close': '263.38000', 'volume': '1899978'}, {'datetime': '2021-02-01 12:30:00', 'open': '261.66000', 'high': '263.35999', 'low': '261.04099', 'close': '262.95999', 'volume': '2047539'}, {'datetime': '2021-02-01 11:30:00', 'open': '258.45001', 'high': '262.72000', 'low': '258.01001', 'close': '261.69501', 'volume': '3227427'}, {'datetime': '2021-02-01 10:30:00', 'open': '259.42001', 'high': '259.64871', 'low': '256.17001', 'close': '258.47989', 'volume': '2743609'}, {'datetime': '2021-02-01 09:30:00', 'open': '259.50000', 'high': '260.87000', 'low': '254.91000', 'close': '259.37000', 'volume': '6033057'}], 'status': 'ok'}
[2023-06-15T09:54:46.852+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-06-15T09:54:46.854+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2023-06-15T09:54:46.854+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2023-06-15T09:54:46.854+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Parents of final stage: List()
[2023-06-15T09:54:46.855+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Missing parents: List()
[2023-06-15T09:54:46.856+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[66] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-06-15T09:54:46.866+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 19.8 KiB, free 434.3 MiB)
[2023-06-15T09:54:46.868+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 434.3 MiB)
[2023-06-15T09:54:46.871+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 69b643fc97df:42739 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:46.872+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2023-06-15T09:54:46.873+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[66] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2023-06-15T09:54:46.874+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2023-06-15T09:54:46.878+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 10) (172.27.0.2, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2023-06-15T09:54:46.878+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 11) (172.27.0.2, executor 0, partition 1, PROCESS_LOCAL, 8084 bytes)
[2023-06-15T09:54:46.910+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.27.0.2:36507 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:46.974+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 11) in 97 ms on 172.27.0.2 (executor 0) (1/2)
[2023-06-15T09:54:46.978+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 10) in 102 ms on 172.27.0.2 (executor 0) (2/2)
[2023-06-15T09:54:46.979+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-06-15T09:54:46.987+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0.129 s
[2023-06-15T09:54:46.988+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-06-15T09:54:46.988+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-06-15T09:54:46.989+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:46 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0.135926 s
[2023-06-15T09:54:48.197+0000] {spark_submit.py:490} INFO - {'meta': {'symbol': 'NFLX', 'interval': '1h', 'currency': 'USD', 'exchange_timezone': 'America/New_York', 'exchange': 'NASDAQ', 'mic_code': 'XNGS', 'type': 'Common Stock'}, 'values': [{'datetime': '2021-02-01 15:30:00', 'open': '542.25238', 'high': '543.58002', 'low': '538.02002', 'close': '538.71002', 'volume': '432616'}, {'datetime': '2021-02-01 14:30:00', 'open': '539.77222', 'high': '542.50000', 'low': '538.29999', 'close': '542.33002', 'volume': '340043'}, {'datetime': '2021-02-01 13:30:00', 'open': '543.78998', 'high': '544.46997', 'low': '539.67999', 'close': '539.82501', 'volume': '289334'}, {'datetime': '2021-02-01 12:30:00', 'open': '540.19000', 'high': '545.05310', 'low': '539.26001', 'close': '543.97998', 'volume': '363749'}, {'datetime': '2021-02-01 11:30:00', 'open': '540.59998', 'high': '541.71997', 'low': '538.50000', 'close': '540.01001', 'volume': '422863'}, {'datetime': '2021-02-01 10:30:00', 'open': '536.43500', 'high': '540.59998', 'low': '533.17999', 'close': '540.32501', 'volume': '412129'}, {'datetime': '2021-02-01 09:30:00', 'open': '537.00000', 'high': '539.19342', 'low': '531.72998', 'close': '536.26001', 'volume': '826354'}], 'status': 'ok'}
[2023-06-15T09:54:48.307+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-06-15T09:54:48.310+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Got job 6 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2023-06-15T09:54:48.310+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Final stage: ResultStage 6 (json at NativeMethodAccessorImpl.java:0)
[2023-06-15T09:54:48.310+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Parents of final stage: List()
[2023-06-15T09:54:48.311+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Missing parents: List()
[2023-06-15T09:54:48.312+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[77] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-06-15T09:54:48.317+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 19.8 KiB, free 434.3 MiB)
[2023-06-15T09:54:48.321+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 434.3 MiB)
[2023-06-15T09:54:48.323+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 69b643fc97df:42739 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:48.323+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
[2023-06-15T09:54:48.325+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[77] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2023-06-15T09:54:48.325+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0
[2023-06-15T09:54:48.327+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (172.27.0.2, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2023-06-15T09:54:48.327+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13) (172.27.0.2, executor 0, partition 1, PROCESS_LOCAL, 8077 bytes)
[2023-06-15T09:54:48.347+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.27.0.2:36507 (size: 9.7 KiB, free: 434.4 MiB)
[2023-06-15T09:54:48.405+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 79 ms on 172.27.0.2 (executor 0) (1/2)
[2023-06-15T09:54:48.407+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 80 ms on 172.27.0.2 (executor 0) (2/2)
[2023-06-15T09:54:48.408+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-06-15T09:54:48.409+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: ResultStage 6 (json at NativeMethodAccessorImpl.java:0) finished in 0.094 s
[2023-06-15T09:54:48.409+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-06-15T09:54:48.409+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-06-15T09:54:48.410+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:48 INFO DAGScheduler: Job 6 finished: json at NativeMethodAccessorImpl.java:0, took 0.101972 s
[2023-06-15T09:54:49.175+0000] {spark_submit.py:490} INFO - {'code': 429, 'message': 'You have run out of API credits for the current minute. 9 API credits were used, with the current limit being 8. Wait for the next minute or consider switching to a higher tier plan at https://twelvedata.com/pricing', 'status': 'error'}
[2023-06-15T09:54:49.176+0000] {spark_submit.py:490} INFO - ERROR:root:Something error
[2023-06-15T09:54:49.292+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO SparkContext: Invoking stop() from shutdown hook
[2023-06-15T09:54:49.292+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-06-15T09:54:49.312+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO SparkUI: Stopped Spark web UI at http://69b643fc97df:4040
[2023-06-15T09:54:49.327+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO StandaloneSchedulerBackend: Shutting down all executors
[2023-06-15T09:54:49.329+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2023-06-15T09:54:49.369+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-06-15T09:54:49.407+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO MemoryStore: MemoryStore cleared
[2023-06-15T09:54:49.411+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO BlockManager: BlockManager stopped
[2023-06-15T09:54:49.419+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-06-15T09:54:49.426+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-06-15T09:54:49.439+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO SparkContext: Successfully stopped SparkContext
[2023-06-15T09:54:49.440+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO ShutdownHookManager: Shutdown hook called
[2023-06-15T09:54:49.441+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2efa075-d06c-4af5-8a44-73220c0bc4db/pyspark-bff9b8b6-bdaa-4bea-a601-a109810f6a5f
[2023-06-15T09:54:49.445+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2efa075-d06c-4af5-8a44-73220c0bc4db
[2023-06-15T09:54:49.449+0000] {spark_submit.py:490} INFO - 23/06/15 09:54:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-2090ba01-cd7d-4c73-b75b-93febd9e5b55
[2023-06-15T09:54:49.556+0000] {taskinstance.py:1345} INFO - Marking task as SUCCESS. dag_id=upload_to_gcs, task_id=spark_load_stock_data, execution_date=20210201T000000, start_date=20230615T095422, end_date=20230615T095449
[2023-06-15T09:54:49.612+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-06-15T09:54:49.632+0000] {taskinstance.py:2651} INFO - 0 downstream tasks scheduled from follow-on schedule check
